---
title: "Day 2: hierarchical and nonlinear models"
description: |
  many groups and curving lines
author:
  - name: Andrew MacDonald 
    affiliation: Universite de Sherbrooke
date: 03-24-2022
output:
  distill::distill_article:
    self_contained: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Learn more about creating blogs with Distill at:
# https://rstudio.github.io/distill/blog.html

```



Outline of today:

* return to previous model: poisson regression 
* panel regression version of this model
* model comparison??? (save for tomorrow)
* brief foray into moment matching
* nonlinear model
* nonlinear model with random  effects


### Quick review


### Bird masses

This example is based on work by Marie-Eve at UdeS! 


We imagine a model like the following: 

$$
\begin{align}
\text{Nestlings}_i & \sim \text{Poisson}(\lambda_i) \\
\text{log}(\lambda_i) &= \beta_0 + \beta_1 \times \text{Mass}_i \\
\beta_0 & \sim \text{Normal}(??, ??) \\
\beta_1 & \sim \text{Normal}(??, ??)
\end{align}
$$


$i$ keeps track of which bird we are talking about. You can think of it as "bird number i"


Note: We could also write the model like this:

$$
\begin{align}
\text{Nestlings}_i & \sim \text{Poisson}(e^{\beta_0} \times e^{\beta_1 \times \text{Mass}_i}) \\
\beta_0 & \sim \text{Normal}(??, ??) \\
\beta_1 & \sim \text{Normal}(??, ??)
\end{align}
$$

### Centering variables

Centering variables is one of the most important things we can do to help our models be more interpretable. This also helps us to set good priors. 

Centering a variable means to subtract the mean from the variable:

$$
\begin{align}
\text{Nestlings}_i & \sim \text{Poisson}(\lambda_i) \\
\text{log}(\lambda_i) &= \beta_0 + \beta_1 \times (\text{Mass}_i - \overline{\text{Mass}}) \\
\beta_0 & \sim \text{Normal}(??, ??) \\
\beta_1 & \sim \text{Normal}(??, ??)
\end{align}
$$
*Question* How does this change the meaning of $\beta_0$ and/or $\beta_1$, if at all? (Hint: what will be the equation for a bird who has exactly average mass?) 


```{r bird-simulation}
set.seed(1234)

n_birds <- 15
avg_nestlings_at_avg_mass <- log(4.2)
effect_of_one_gram <- .2

mother_masses_g <- rnorm(n_birds, mean = 15, sd = 3)
avg_mother_mass <- mean(mother_masses_g)

log_average_nestlings <- avg_nestlings_at_avg_mass + 
  effect_of_one_gram * (mother_masses_g - avg_mother_mass)

nestlings <- rpois(n = n_birds, lambda = exp(log_average_nestlings))
```

Plot these to get an idea of it:

```{r}
suppressPackageStartupMessages(library(tidyverse))
imaginary_birds <- tibble(mother_masses_g, nestlings)

ggplot(imaginary_birds, aes(x = mother_masses_g, y = nestlings)) + 
  geom_point()
```

*NOTE* We can also fit this very same model by frequentist statistics, using `lm`

```{r}
coef(glm(nestlings ~ 1 + I(mother_masses_g - mean(mother_masses_g)), family = "poisson"))
# compare to known values
avg_nestlings_at_avg_mass
effect_of_one_gram
```

### Bayesian workflow: define a model and priors

```{r}
library(brms)

imaginary_birds_centered <- imaginary_birds |> 
  mutate(mother_mass_g_cen = mother_masses_g - mean(mother_masses_g))

bird_form <- bf(nestlings ~ 1 + mother_mass_g_cen, family = poisson(link = "log"))

get_prior(bird_form, data = imaginary_birds_centered)
```

We set a prior for each parameter. 

```{r}
bird_priors <- c(
  prior(normal(1, .5), class = "Intercept"),
  prior(normal(.1, .1), class = "b", coef = "mother_mass_g_cen")
)
```

#### prior predictive checks

```{r}
prior_predictions <- brm(bird_form,
                         data = imaginary_birds_centered,
                         prior = bird_priors, 
                         sample_prior = "only", 
                         file = "bird_model",
                         file_refit = "on_change",
                         refresh = FALSE)
```


plot a few of these

```{r}
library(tidybayes)
imaginary_birds_centered |> 
  add_predicted_draws(prior_predictions, ndraws = 6, seed = 4321) |> 
  ggplot(aes(x = mother_masses_g, y = .prediction)) + geom_point() + facet_wrap(~.draw)
```

*Question* are we satisfied with these priors?

#### Fit to the data

```{r}
bird_posterior <- update(prior_predictions, sample_prior = "yes", 
                         file = "bird_posterior", 
                         file_refit = "on_change", refresh = FALSE)
```


```{r}
summary(bird_posterior)

knitr::kable(head(tidybayes::tidy_draws(bird_posterior)))
```


How do our priors and posteriors compare?

```{r}
library(ggridges)
tidybayes::tidy_draws(bird_posterior) |> 
  select(.draw, b_Intercept:prior_b_mother_mass_g_cen) |> 
  pivot_longer(-.draw) |> 
  ggplot(aes(x = value, y = name)) + geom_density_ridges()
```

Can we draw the regression line? 

```{r}
average_mom <- mean(mother_masses_g)

range(imaginary_birds_centered$mother_mass_g_cen)

tibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, 
                                             n = 10)) |> 
  tidybayes::add_epred_draws(bird_posterior) |> 
  ungroup() |> 
  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .epred)) + 
  stat_lineribbon() + 
  scale_fill_brewer(palette = "Greens", direction = -1) + 
  geom_point(aes(x = mother_masses_g, y = nestlings),
             data = imaginary_birds_centered, pch = 21,
             fill = "orange", size = 3)
  

```

let's also try drawing the prediction intervals

```{r}
average_mom <- mean(mother_masses_g)

range(imaginary_birds_centered$mother_mass_g_cen)

tibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, 
                                             n = 10)) |> 
  tidybayes::add_predicted_draws(bird_posterior) |> 
  ungroup() |> 
  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .prediction)) + 
  stat_lineribbon() + 
  scale_fill_brewer(palette = "Greens", direction = -1) + 
  geom_point(aes(x = mother_masses_g, y = nestlings),
             data = imaginary_birds_centered, pch = 21,
             fill = "orange", size = 3)
  

```

