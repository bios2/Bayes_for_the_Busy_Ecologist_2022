[
  {
    "path": "posts/2022-03-29-day3-nonlinear-models/",
    "title": "Day3: nonlinear models",
    "description": "Nature has no straight lines",
    "author": [
      {
        "name": "Andrew MacDonald",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\r\nFitting nonlinear models to ecological data is interesting and powerful. This is possible in base R using the function nls(). In a Bayesian approach we can do the same thing, but we don’t need to learn any new tools.\r\nHemlock growth\r\nwe will work with a dataset\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidybayes)\r\nlibrary(brms)\r\n\r\nhemlock <- readr::read_delim(\r\n  \"https://raw.githubusercontent.com/bios2/biodiversity_modelling_2021/master/data/hemlock.txt\", \r\n  delim = \" \",\r\n  col_names = c(\"x\",\"light\", \"growth\"), skip = 1)\r\n\r\nknitr::kable(head(hemlock, n = 3))\r\n\r\n\r\nx\r\nlight\r\ngrowth\r\n1\r\n32.34929\r\n118.2052\r\n2\r\n58.84066\r\n138.0278\r\n3\r\n75.05452\r\n185.7844\r\n\r\nggplot(hemlock, aes(x = light, y = growth)) + \r\n  geom_point()\r\n\r\n\r\n\r\n\r\ndefine a model\r\nWe need a function for the mean growth rate per species. A very popular choice in ecology is the famous Type 2 functional response:\r\n\\[\r\ny = \\frac{a x}{b + x}\r\n\\] * \\(a\\) is the asymptote – the max value of \\(y\\) when \\(x\\) is large * \\(b\\) is the value of \\(x\\) where \\(y = a/2\\)\r\nWe experiment using curve to understand how this works\r\n\r\n\r\na <- 195\r\nb <- 30\r\ncurve(a * x / (b + x), xlim = c(0, 100))\r\n\r\n\r\n\r\n\r\ndefine a distribution for observations around this average\r\nWe can use the gamma distribution. The gamma distribution looks like this:\r\n\r\n\r\ncurve(dgamma(x, 3,5), xlim = c(0, 3))\r\n\r\n\r\n\r\n\r\nThe gamma distribution has two parameters:\r\n\\[\r\n\\text{Gamma}(a, b)\r\n\\]\r\nThe mean and variance are both functions of both of these parameters:\r\n\\[\r\n\\mu = \\frac{a}{b}\r\n\\] \\[\r\n\\sigma = \\frac{a}{b^2}\r\n\\]\r\nwe can demonstrate this easily in R:\r\n\r\n\r\nxx <- rgamma(5000, 3, 5)\r\nmean(xx) #about 3/5  = 0.6\r\n\r\n\r\n[1] 0.596843\r\n\r\nvar(xx) #about 3/(5^2) = 0.12\r\n\r\n\r\n[1] 0.117797\r\n\r\nWe can reverse this: write the parameters \\(a\\) and \\(b\\) in terms of the desired mean and standard deviation:\r\n\\[\r\n\\begin{align}\r\na &= \\frac{\\mu^2}{\\sigma^2} \\\\\r\nb &= \\frac{\\mu}{\\sigma^2} \\\\\r\n\\end{align}\r\n\\]\r\noptional prove that to yourself with algebra!\r\nexercise simulate 3000 from a Gamma distribution with a mean of 42 and a standard deviation of 10\r\nsimulating observations:\r\nwe exploit this technique to make up some fake data:\r\n\r\n\r\na <- 195\r\nb <- 20\r\nx_values <- runif(n = 70, min = 0, max = 100)\r\naverage_response <- a * x_values / (b + x_values)\r\nplot(x_values, average_response)\r\n\r\n\r\n\r\n\r\n\r\n\r\nsigma <- 15\r\n\r\nobserved_response <- rgamma(n = 70, shape = average_response^2/sigma^2, rate = average_response/ sigma^2)\r\n\r\nplot(x_values, observed_response)\r\n\r\n\r\n\r\n\r\nDefining a bayesian model\r\nTo fully build our Bayesian model we put all the above together:\r\nour function that describes how light causes the average growth\r\na distribution for observations around that average\r\npriors for the three unobserved quantities: \\(a\\), \\(b\\) and \\(\\sigma\\)\r\n\\[\r\n\\begin{align}\r\n\\text{height} &\\sim \\text{Gamma}(\\mu^2/\\sigma^2, \\mu/\\sigma^2) \\\\\r\n\\mu &= \\frac{aL}{b + L} \\\\\r\n\\sigma_{height} & \\sim \\text{Exponential}(4)\\\\\r\na               & \\sim \\text{Normal}(200, 15)\\\\\r\nb               & \\sim \\text{Normal}(25, 5)\\\\\r\n\\end{align}\r\n\\] ## Prior predictive simulations\r\n\r\n\r\nhemlock$x <- NULL\r\n\r\nlight_curve_bf <- bf(growth ~ a * light / (b + light),\r\n                        family = Gamma(link = \"identity\"),\r\n                        a ~ 1,\r\n                        b ~ 1, \r\n                        nl = TRUE)\r\n\r\nget_prior(light_curve_bf, data = hemlock)\r\n\r\n\r\n             prior class      coef group resp dpar nlpar bound\r\n gamma(0.01, 0.01) shape                                      \r\n            (flat)     b                               a      \r\n            (flat)     b Intercept                     a      \r\n            (flat)     b                               b      \r\n            (flat)     b Intercept                     b      \r\n       source\r\n      default\r\n      default\r\n (vectorized)\r\n      default\r\n (vectorized)\r\n\r\nlight_curve_prior <- c(\r\n  # prior(exponential(.1), class = \"shape\"),\r\n  prior(gamma(6.25, .25), class = \"shape\"),\r\n  prior(normal(250, 50), class = \"b\", nlpar = \"a\"),\r\n  prior(normal(30, 10), class = \"b\", nlpar = \"b\")\r\n)\r\n\r\nlight_curve_model_prior <- brm(light_curve_bf,\r\n                               prior = light_curve_prior,\r\n                               data = hemlock,\r\n                               refresh = FALSE,\r\n                               sample_prior = \"only\", \r\n                               file = here::here(\"_posts\", \"2022-03-29-day3-nonlinear-models\", \"light_curve_prior\"),\r\n                               file_refit = \"on_change\")\r\n\r\n\r\n\r\n\r\n\r\nhemlock |> \r\n  add_predicted_draws(light_curve_model_prior, ndraws = 6) |> \r\n  ggplot(aes(x = light, y = .prediction)) + \r\n  geom_point() + \r\n  facet_wrap(~.draw) + \r\n  coord_cartesian(ylim = c(0, 300))\r\n\r\n\r\n\r\n\r\nfit to real data:\r\n\r\n\r\nlight_curve_model_posterior <- brm(light_curve_bf,\r\n                               prior = light_curve_prior,\r\n                               data = hemlock,\r\n                               sample_prior = \"only\", \r\n                               file = here::here(\"_posts\", \"2022-03-29-day3-nonlinear-models\", \"light_curve_posterior\"),\r\n                               file_refit = \"on_change\")\r\n\r\n\r\n\r\nPredictions with the original data:\r\n\r\n\r\nhemlock_post <- hemlock |> \r\n  add_predicted_draws(light_curve_model_posterior)\r\n\r\n\r\nhemlock_post |> \r\n  ggplot(aes(x = light, y = .prediction)) + \r\n  stat_lineribbon()+ \r\n  coord_cartesian(ylim = c(0, 300)) + \r\n  scale_fill_brewer(palette = \"Oranges\") + \r\n  geom_point(aes(x = light, y = growth), size = 3, pch = 21, fill = \"lightblue\", data = hemlock)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-29-day3-nonlinear-models/day3-nonlinear-models_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-03-29T13:18:47-04:00",
    "input_file": "day3-nonlinear-models.knit.md"
  },
  {
    "path": "posts/2022-03-24-day-2-hierarchical-and-nonlinear-models/",
    "title": "Day 2: hierarchical and nonlinear models",
    "description": "many groups and curving lines",
    "author": [
      {
        "name": "Andrew MacDonald",
        "url": {}
      }
    ],
    "date": "2022-03-24",
    "categories": [],
    "contents": "\r\nOutline of today:\r\nreturn to previous model: poisson regression\r\npanel regression version of this model\r\nmodel comparison??? (save for tomorrow)\r\nbrief foray into moment matching\r\nnonlinear model\r\nnonlinear model with random effects\r\nQuick review\r\nBird masses\r\nThis example is based on work by Marie-Eve at UdeS!\r\nWe imagine a model like the following:\r\n\\[\r\n\\begin{align}\r\n\\text{Nestlings}_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\r\n\\text{log}(\\lambda_i) &= \\beta_0 + \\beta_1 \\times \\text{Mass}_i \\\\\r\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\r\n\\beta_1 & \\sim \\text{Normal}(??, ??)\r\n\\end{align}\r\n\\]\r\n\\(i\\) keeps track of which bird we are talking about. You can think of it as “bird number i”\r\nNote: We could also write the model like this:\r\n\\[\r\n\\begin{align}\r\n\\text{Nestlings}_i & \\sim \\text{Poisson}(e^{\\beta_0} \\times e^{\\beta_1 \\times \\text{Mass}_i}) \\\\\r\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\r\n\\beta_1 & \\sim \\text{Normal}(??, ??)\r\n\\end{align}\r\n\\]\r\nCentering variables\r\nCentering variables is one of the most important things we can do to help our models be more interpretable. This also helps us to set good priors.\r\nCentering a variable means to subtract the mean from the variable:\r\n\\[\r\n\\begin{align}\r\n\\text{Nestlings}_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\r\n\\text{log}(\\lambda_i) &= \\beta_0 + \\beta_1 \\times (\\text{Mass}_i - \\overline{\\text{Mass}}) \\\\\r\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\r\n\\beta_1 & \\sim \\text{Normal}(??, ??)\r\n\\end{align}\r\n\\] Question How does this change the meaning of \\(\\beta_0\\) and/or \\(\\beta_1\\), if at all? (Hint: what will be the equation for a bird who has exactly average mass?)\r\n\r\n\r\nset.seed(1234)\r\n\r\nn_birds <- 15\r\navg_nestlings_at_avg_mass <- log(4.2)\r\neffect_of_one_gram <- .2\r\n\r\nmother_masses_g <- rnorm(n_birds, mean = 15, sd = 3)\r\navg_mother_mass <- mean(mother_masses_g)\r\n\r\nlog_average_nestlings <- avg_nestlings_at_avg_mass + \r\n  effect_of_one_gram * (mother_masses_g - avg_mother_mass)\r\n\r\nnestlings <- rpois(n = n_birds, lambda = exp(log_average_nestlings))\r\n\r\n\r\n\r\nPlot these to get an idea of it:\r\n\r\n\r\nsuppressPackageStartupMessages(library(tidyverse))\r\nimaginary_birds <- tibble(mother_masses_g, nestlings)\r\n\r\nggplot(imaginary_birds, aes(x = mother_masses_g, y = nestlings)) + \r\n  geom_point()\r\n\r\n\r\n\r\n\r\nNOTE We can also fit this very same model by frequentist statistics, using lm\r\n\r\n\r\ncoef(glm(nestlings ~ 1 + I(mother_masses_g - mean(mother_masses_g)), family = \"poisson\"))\r\n\r\n\r\n                               (Intercept) \r\n                                 1.4138103 \r\nI(mother_masses_g - mean(mother_masses_g)) \r\n                                 0.1727791 \r\n\r\n# compare to known values\r\navg_nestlings_at_avg_mass\r\n\r\n\r\n[1] 1.435085\r\n\r\neffect_of_one_gram\r\n\r\n\r\n[1] 0.2\r\n\r\nBayesian workflow: define a model and priors\r\n\r\n\r\nlibrary(brms)\r\n\r\nimaginary_birds_centered <- imaginary_birds |> \r\n  mutate(mother_mass_g_cen = mother_masses_g - mean(mother_masses_g))\r\n\r\nbird_form <- bf(nestlings ~ 1 + mother_mass_g_cen, family = poisson(link = \"log\"))\r\n\r\nget_prior(bird_form, data = imaginary_birds_centered)\r\n\r\n\r\n                  prior     class              coef group resp dpar\r\n                 (flat)         b                                  \r\n                 (flat)         b mother_mass_g_cen                \r\n student_t(3, 1.4, 2.5) Intercept                                  \r\n nlpar bound       source\r\n                  default\r\n             (vectorized)\r\n                  default\r\n\r\nWe set a prior for each parameter.\r\n\r\n\r\nbird_priors <- c(\r\n  prior(normal(1, .5), class = \"Intercept\"),\r\n  prior(normal(.1, .1), class = \"b\", coef = \"mother_mass_g_cen\")\r\n)\r\n\r\n\r\n\r\nprior predictive checks\r\n\r\n\r\nprior_predictions <- brm(bird_form,\r\n                         data = imaginary_birds_centered,\r\n                         prior = bird_priors, \r\n                         sample_prior = \"only\", \r\n                         file = \"bird_model\",\r\n                         file_refit = \"on_change\",\r\n                         refresh = FALSE)\r\n\r\n\r\n\r\nplot a few of these\r\n\r\n\r\nlibrary(tidybayes)\r\nimaginary_birds_centered |> \r\n  add_predicted_draws(prior_predictions, ndraws = 6, seed = 4321) |> \r\n  ggplot(aes(x = mother_masses_g, y = .prediction)) + geom_point() + facet_wrap(~.draw)\r\n\r\n\r\n\r\n\r\nQuestion are we satisfied with these priors?\r\nFit to the data\r\n\r\n\r\nbird_posterior <- update(prior_predictions, sample_prior = \"yes\", \r\n                         file = \"bird_posterior\", \r\n                         file_refit = \"on_change\", refresh = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nsummary(bird_posterior)\r\n\r\n\r\n Family: poisson \r\n  Links: mu = log \r\nFormula: nestlings ~ 1 + mother_mass_g_cen \r\n   Data: imaginary_birds_centered (Number of observations: 15) \r\n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 4000\r\n\r\nPopulation-Level Effects: \r\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept             1.39      0.13     1.13     1.64 1.00     2613\r\nmother_mass_g_cen     0.16      0.04     0.08     0.25 1.00     2536\r\n                  Tail_ESS\r\nIntercept             2494\r\nmother_mass_g_cen     2402\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nknitr::kable(head(tidybayes::tidy_draws(bird_posterior)))\r\n\r\n\r\n.chain\r\n.iteration\r\n.draw\r\nb_Intercept\r\nb_mother_mass_g_cen\r\nprior_Intercept\r\nprior_b_mother_mass_g_cen\r\nlprior\r\nlp__\r\naccept_stat__\r\nstepsize__\r\ntreedepth__\r\nn_leapfrog__\r\ndivergent__\r\nenergy__\r\n1\r\n1\r\n1\r\n1.643954\r\n0.1813006\r\n0.5520973\r\n0.1319884\r\n-0.0019872\r\n-31.73046\r\n0.8433474\r\n0.819504\r\n2\r\n3\r\n0\r\n33.76309\r\n1\r\n2\r\n2\r\n1.140257\r\n0.1877014\r\n1.3621787\r\n0.0842671\r\n0.7339344\r\n-30.96103\r\n1.0000000\r\n0.819504\r\n2\r\n3\r\n0\r\n32.63675\r\n1\r\n3\r\n3\r\n1.060480\r\n0.1662869\r\n0.4521973\r\n-0.1016976\r\n0.9308422\r\n-32.60084\r\n0.7210825\r\n0.819504\r\n1\r\n1\r\n0\r\n32.68563\r\n1\r\n4\r\n4\r\n1.138671\r\n0.1571804\r\n1.0768996\r\n0.2397268\r\n0.9559159\r\n-31.31604\r\n1.0000000\r\n0.819504\r\n1\r\n1\r\n0\r\n32.62936\r\n1\r\n5\r\n5\r\n1.443166\r\n0.2059157\r\n1.4607618\r\n0.0400797\r\n0.2041559\r\n-29.77087\r\n0.9950998\r\n0.819504\r\n2\r\n7\r\n0\r\n31.04836\r\n1\r\n6\r\n6\r\n1.545824\r\n0.1237708\r\n1.3101264\r\n0.0343844\r\n0.5337555\r\n-29.80001\r\n0.8797944\r\n0.819504\r\n2\r\n5\r\n0\r\n31.09643\r\n\r\nHow do our priors and posteriors compare?\r\n\r\n\r\nlibrary(ggridges)\r\ntidybayes::tidy_draws(bird_posterior) |> \r\n  select(.draw, b_Intercept:prior_b_mother_mass_g_cen) |> \r\n  pivot_longer(-.draw) |> \r\n  ggplot(aes(x = value, y = name)) + geom_density_ridges()\r\n\r\n\r\n\r\n\r\nCan we draw the regression line?\r\n\r\n\r\naverage_mom <- mean(mother_masses_g)\r\n\r\nrange(imaginary_birds_centered$mother_mass_g_cen)\r\n\r\n\r\n[1] -6.025202  4.265214\r\n\r\ntibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, \r\n                                             n = 10)) |> \r\n  tidybayes::add_epred_draws(bird_posterior) |> \r\n  ungroup() |> \r\n  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .epred)) + \r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \r\n  geom_point(aes(x = mother_masses_g, y = nestlings),\r\n             data = imaginary_birds_centered, pch = 21,\r\n             fill = \"orange\", size = 3)\r\n\r\n\r\n\r\n\r\nlet’s also try drawing the prediction intervals\r\n\r\n\r\naverage_mom <- mean(mother_masses_g)\r\n\r\nrange(imaginary_birds_centered$mother_mass_g_cen)\r\n\r\n\r\n[1] -6.025202  4.265214\r\n\r\ntibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, \r\n                                             n = 10)) |> \r\n  tidybayes::add_predicted_draws(bird_posterior) |> \r\n  ungroup() |> \r\n  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .prediction)) + \r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \r\n  geom_point(aes(x = mother_masses_g, y = nestlings),\r\n             data = imaginary_birds_centered, pch = 21,\r\n             fill = \"orange\", size = 3)\r\n\r\n\r\n\r\n\r\nOther checks we can do:\r\n\r\n\r\nbird_posterior_onlyparam <- update(prior_predictions, sample_prior = \"no\", \r\n                         file = \"bird_posterior\", \r\n                         file_refit = \"on_change\", refresh = FALSE)\r\n\r\nshinystan::launch_shinystan(bird_posterior_onlyparam)\r\n\r\n\r\n\r\nMultilevel models\r\nBased on the awesome vignette for vignette for tidybayes\r\nWe begin by sampling some data from five different “conditions”:\r\n\r\n\r\nlibrary(modelr)\r\nset.seed(5)\r\nn <- 10\r\nn_condition <- 5\r\nABC <-\r\n  data_frame(\r\n    condition = rep(c(\"A\", \"B\", \"C\", \"D\", \"E\"), n),\r\n    response = rnorm(n * 5, c(0, 1, 2, 1, -1), 0.5)\r\n  )\r\n\r\nABC %>%\r\n  ggplot(aes(y = condition, x = response)) +\r\n  geom_point(pch = 21, size = 4, stroke = 1.4, fill = \"#41b6c4\")\r\n\r\n\r\n\r\n\r\nAnd by fitting a model to these data, with varying intercepts for each group:\r\n\r\n\r\nm <- brm(\r\n  response ~ (1 | condition), data = ABC, \r\n  control = list(adapt_delta = .99),\r\n  prior = c(\r\n    prior(normal(0, 1), class = Intercept),\r\n    prior(student_t(3, 0, 1), class = sd),\r\n    prior(student_t(3, 0, 1), class = sigma)\r\n  )\r\n)\r\n\r\n\r\n\r\nAn easy way to visualize these results is with a ridgeline plot as above\r\n\r\n\r\nABC %>%\r\n  modelr::data_grid(condition) %>%\r\n  tidybayes::add_predicted_draws(m) %>%\r\n  ggplot(aes(x = .prediction, y = condition)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") + \r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nAlright. This used the simple vanilla option, add_predicted_samples(m). This uses the default options for making predictions, which recall is “NULL (default), include all group-level effects”. If you set add_predicted_samples(m, re_formula = NULL), you’ll get exactly the same figure.\r\nSo we can see that to “include” an effect is to take the actual estimated intercepts for each specific group we studied and use them to make new predictions for the same groups. This is Case 1 from McElreath’s list (though in this case, because we only have groups and nothing else, Case 1 and 2 are the same).\r\nWe can also say the exact same thing using a formula:\r\n\r\n\r\nABC %>%\r\n  data_grid(condition) %>%\r\n  add_predicted_draws(m, re_formula = ~(1|condition)) %>%\r\n  ggplot(aes(x = .prediction, y = condition)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") +  \r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nThat’s right, there are three ways to say the exact same thing: say nothing, say NULL, or say the original “random effects” formula1. You go with what you feel in your heart is right, but I prefer the formula.\r\nIn all three cases, we are using the model to predict the means for the groups in our varying-intercepts model. This is what the documentation means by “including” these varying intercepts.\r\nSquishing those random effects\r\nOK so that was three separate ways to make predictions for the same groups. What else can we do? Let’s try that thing with the NA argument, which means “include no group-level effects”:\r\n\r\n\r\nABC %>%\r\n  data_grid(condition) %>%\r\n  add_predicted_draws(m, re_formula = NA,\r\n                        n = 2000) %>%\r\n  ggplot(aes(x = .prediction, y = condition)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") +    theme_minimal()\r\n\r\n\r\n\r\n\r\nAh, so if you do this, all the groups come out the same! But if they’re all the same, what do they represent? It seems reasonable that they represent the model’s intercept, as if the varying intercepts were all 0. Let’s calculate predicitons that ignore the varying effects – that is, using only the model intercept and the standard deviation of the response – using a bit of [handy purrr magic]2:\r\n\r\n\r\nm %>% \r\n  spread_draws(b_Intercept, sigma) %>% \r\n  mutate(prediction = rnorm(length(b_Intercept), b_Intercept, sigma),\r\n         #map2_dbl(b_Intercept, sigma, ~ rnorm(1, mean = .x, sd = .y)),\r\n         Prediction = \"prediction\") %>% #glimpse %>% \r\n  ggplot(aes(x = prediction, y = Prediction)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") +    \r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nAs you can see, this distribution has exactly the same shape as the five in the previous figure! It is as if we calculated the predictions for a group which was exactly at the average (in other words, it had a varying intercept of 0.) In the Rethinking book, readers are taught to do this in a much more explicit way: you actually generate all the 0 intercepts yourself, and give that to the model in place of the estimated intercepts! A very manual and concrete way to “set something to 0”.\r\nbrms does this too. As the documentation says >NA values within factors in newdata, are interpreted as if all dummy variables of this factor are zero.\r\nThe brms phrasing certainly takes less space, though it also requires you to remember that this is what NA gets you!\r\nWe can also remove random effects from our predictions by excluding them from the re_formula. In our model, we have only one varying effect – yet an even simpler formula is possible, a model with no intercept at all:\r\n\r\n\r\nABC %>%\r\n  data_grid(condition) %>%\r\n  add_predicted_draws(m, re_formula = ~ 0,\r\n                        n = 2000) %>%\r\n  ggplot(aes(x = .prediction, y = condition)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") + theme_minimal() \r\n\r\n\r\n\r\n\r\nOnce again, the same distribution appears: it is as if all group effects had been set to zero. If we had two random effects and omitted one, this is what we would get for the omitted effect – the expected value if all its effects were 0.\r\nNew levels\r\nI’m going to show how to create predictions for new levels, but first I’m going to show two mistakes that I made frequently while learning:\r\nFirst, asking for new levels without specifying allow_new_levels = TRUE:\r\n\r\n\r\n# this does not work at all!!\r\ndata_frame(condition = \"bugaboo\") %>%\r\n  add_predicted_draws(m, re_formula = ~(1|condition),\r\n                        n = 2000)\r\n\r\n\r\nError: Levels 'bugaboo' of grouping factor 'condition' cannot be found in the fitted model. Consider setting argument 'allow_new_levels' to TRUE.\r\n\r\nThat fails because I tried to pass in a level of my grouping variable that wasn’t in the original model!\r\nSecond, passing in new levels – but telling the function to just ignore them:\r\n\r\n\r\ndata_frame(condition = \"bugaboo\") %>%\r\n  add_predicted_draws(m, re_formula = NA,#~(1|condition),\r\n                        n = 2000) %>%\r\n  ggplot(aes(x = .prediction, y = condition)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") + \r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nHere, i’m still passing in the unknown level – but the function doesn’t complain, because I’m not including random effects at all! This is the same result from above, when we used NA or ~0 to remove varying effects altogether. This is definitely something to watch for if you are passing in new data (I made this mistake, and it cost me an afternoon!)\r\nIf we avoid both of these errors, we get what we expect: our means for our original groups, and a new predicted mean for \"bugaboo\":\r\n\r\n\r\nABC %>%\r\n  data_grid(condition) %>% \r\n  add_row(condition = \"bugaboo\") %>%\r\n  add_predicted_draws(m, re_formula = ~(1|condition),\r\n                        allow_new_levels = TRUE,\r\n                        n = 2000) %>%\r\n  ggplot(aes(x = .prediction, y = condition)) +\r\n  geom_density_ridges(fill = \"#41b6c4\") +    theme_minimal()\r\n\r\n\r\n\r\n\r\nHere you can see that the new level is much flatter than the other original five. It comes from the same population as the others, which is rather variable (the group means are sort of different to each other). As a result, this new distribution is quite wide, including all that uncertainty.\r\nAn ecologist might do something like this if we were had data on some species in a community, but wanted to make predictions for new, as yet unobserved, species we might find next year.\r\n\r\nthis impulse in R to “help your users” by making it possible to say a great deal by saying almost nothing is… actually pretty counterproductive, I’d argue? But that’s a different post↩︎\r\nno magic required! rnorm is already vectorized↩︎\r\n",
    "preview": "posts/2022-03-24-day-2-hierarchical-and-nonlinear-models/Day2_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-03-24T15:31:59-04:00",
    "input_file": "Day2.knit.md"
  }
]
