[
  {
    "path": "posts/2022-03-24-day-2-hierarchical-and-nonlinear-models/",
    "title": "Day 2: hierarchical and nonlinear models",
    "description": "many groups and curving lines",
    "author": [
      {
        "name": "Andrew MacDonald",
        "url": {}
      }
    ],
    "date": "2022-03-24",
    "categories": [],
    "contents": "\r\nOutline of today:\r\nreturn to previous model: poisson regression\r\npanel regression version of this model\r\nmodel comparison??? (save for tomorrow)\r\nbrief foray into moment matching\r\nnonlinear model\r\nnonlinear model with random effects\r\nQuick review\r\nBird masses\r\nThis example is based on work by Marie-Eve at UdeS!\r\nWe imagine a model like the following:\r\n\\[\r\n\\begin{align}\r\n\\text{Nestlings}_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\r\n\\text{log}(\\lambda_i) &= \\beta_0 + \\beta_1 \\times \\text{Mass}_i \\\\\r\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\r\n\\beta_1 & \\sim \\text{Normal}(??, ??)\r\n\\end{align}\r\n\\]\r\n\\(i\\) keeps track of which bird we are talking about. You can think of it as “bird number i”\r\nNote: We could also write the model like this:\r\n\\[\r\n\\begin{align}\r\n\\text{Nestlings}_i & \\sim \\text{Poisson}(e^{\\beta_0} \\times e^{\\beta_1 \\times \\text{Mass}_i}) \\\\\r\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\r\n\\beta_1 & \\sim \\text{Normal}(??, ??)\r\n\\end{align}\r\n\\]\r\nCentering variables\r\nCentering variables is one of the most important things we can do to help our models be more interpretable. This also helps us to set good priors.\r\nCentering a variable means to subtract the mean from the variable:\r\n\\[\r\n\\begin{align}\r\n\\text{Nestlings}_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\r\n\\text{log}(\\lambda_i) &= \\beta_0 + \\beta_1 \\times (\\text{Mass}_i - \\overline{\\text{Mass}}) \\\\\r\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\r\n\\beta_1 & \\sim \\text{Normal}(??, ??)\r\n\\end{align}\r\n\\] Question How does this change the meaning of \\(\\beta_0\\) and/or \\(\\beta_1\\), if at all? (Hint: what will be the equation for a bird who has exactly average mass?)\r\n\r\n\r\nset.seed(1234)\r\n\r\nn_birds <- 15\r\navg_nestlings_at_avg_mass <- log(4.2)\r\neffect_of_one_gram <- .2\r\n\r\nmother_masses_g <- rnorm(n_birds, mean = 15, sd = 3)\r\navg_mother_mass <- mean(mother_masses_g)\r\n\r\nlog_average_nestlings <- avg_nestlings_at_avg_mass + \r\n  effect_of_one_gram * (mother_masses_g - avg_mother_mass)\r\n\r\nnestlings <- rpois(n = n_birds, lambda = exp(log_average_nestlings))\r\n\r\n\r\n\r\nPlot these to get an idea of it:\r\n\r\n\r\nsuppressPackageStartupMessages(library(tidyverse))\r\nimaginary_birds <- tibble(mother_masses_g, nestlings)\r\n\r\nggplot(imaginary_birds, aes(x = mother_masses_g, y = nestlings)) + \r\n  geom_point()\r\n\r\n\r\n\r\n\r\nNOTE We can also fit this very same model by frequentist statistics, using lm\r\n\r\n\r\ncoef(glm(nestlings ~ 1 + I(mother_masses_g - mean(mother_masses_g)), family = \"poisson\"))\r\n\r\n\r\n                               (Intercept) \r\n                                 1.4138103 \r\nI(mother_masses_g - mean(mother_masses_g)) \r\n                                 0.1727791 \r\n\r\n# compare to known values\r\navg_nestlings_at_avg_mass\r\n\r\n\r\n[1] 1.435085\r\n\r\neffect_of_one_gram\r\n\r\n\r\n[1] 0.2\r\n\r\nBayesian workflow: define a model and priors\r\n\r\n\r\nlibrary(brms)\r\n\r\nimaginary_birds_centered <- imaginary_birds |> \r\n  mutate(mother_mass_g_cen = mother_masses_g - mean(mother_masses_g))\r\n\r\nbird_form <- bf(nestlings ~ 1 + mother_mass_g_cen, family = poisson(link = \"log\"))\r\n\r\nget_prior(bird_form, data = imaginary_birds_centered)\r\n\r\n\r\n                  prior     class              coef group resp dpar\r\n                 (flat)         b                                  \r\n                 (flat)         b mother_mass_g_cen                \r\n student_t(3, 1.4, 2.5) Intercept                                  \r\n nlpar bound       source\r\n                  default\r\n             (vectorized)\r\n                  default\r\n\r\nWe set a prior for each parameter.\r\n\r\n\r\nbird_priors <- c(\r\n  prior(normal(1, .5), class = \"Intercept\"),\r\n  prior(normal(.1, .1), class = \"b\", coef = \"mother_mass_g_cen\")\r\n)\r\n\r\n\r\n\r\nprior predictive checks\r\n\r\n\r\nprior_predictions <- brm(bird_form,\r\n                         data = imaginary_birds_centered,\r\n                         prior = bird_priors, \r\n                         sample_prior = \"only\", \r\n                         file = \"bird_model\",\r\n                         file_refit = \"on_change\",\r\n                         refresh = FALSE)\r\n\r\n\r\n\r\nplot a few of these\r\n\r\n\r\nlibrary(tidybayes)\r\nimaginary_birds_centered |> \r\n  add_predicted_draws(prior_predictions, ndraws = 6, seed = 4321) |> \r\n  ggplot(aes(x = mother_masses_g, y = .prediction)) + geom_point() + facet_wrap(~.draw)\r\n\r\n\r\n\r\n\r\nQuestion are we satisfied with these priors?\r\nFit to the data\r\n\r\n\r\nbird_posterior <- update(prior_predictions, sample_prior = \"yes\", \r\n                         file = \"bird_posterior\", \r\n                         file_refit = \"on_change\", refresh = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nsummary(bird_posterior)\r\n\r\n\r\n Family: poisson \r\n  Links: mu = log \r\nFormula: nestlings ~ 1 + mother_mass_g_cen \r\n   Data: imaginary_birds_centered (Number of observations: 15) \r\n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 4000\r\n\r\nPopulation-Level Effects: \r\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept             1.39      0.13     1.13     1.64 1.00     2613\r\nmother_mass_g_cen     0.16      0.04     0.08     0.25 1.00     2536\r\n                  Tail_ESS\r\nIntercept             2494\r\nmother_mass_g_cen     2402\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nknitr::kable(head(tidybayes::tidy_draws(bird_posterior)))\r\n\r\n\r\n.chain\r\n.iteration\r\n.draw\r\nb_Intercept\r\nb_mother_mass_g_cen\r\nprior_Intercept\r\nprior_b_mother_mass_g_cen\r\nlprior\r\nlp__\r\naccept_stat__\r\nstepsize__\r\ntreedepth__\r\nn_leapfrog__\r\ndivergent__\r\nenergy__\r\n1\r\n1\r\n1\r\n1.643954\r\n0.1813006\r\n0.5520973\r\n0.1319884\r\n-0.0019872\r\n-31.73046\r\n0.8433474\r\n0.819504\r\n2\r\n3\r\n0\r\n33.76309\r\n1\r\n2\r\n2\r\n1.140257\r\n0.1877014\r\n1.3621787\r\n0.0842671\r\n0.7339344\r\n-30.96103\r\n1.0000000\r\n0.819504\r\n2\r\n3\r\n0\r\n32.63675\r\n1\r\n3\r\n3\r\n1.060480\r\n0.1662869\r\n0.4521973\r\n-0.1016976\r\n0.9308422\r\n-32.60084\r\n0.7210825\r\n0.819504\r\n1\r\n1\r\n0\r\n32.68563\r\n1\r\n4\r\n4\r\n1.138671\r\n0.1571804\r\n1.0768996\r\n0.2397268\r\n0.9559159\r\n-31.31604\r\n1.0000000\r\n0.819504\r\n1\r\n1\r\n0\r\n32.62936\r\n1\r\n5\r\n5\r\n1.443166\r\n0.2059157\r\n1.4607618\r\n0.0400797\r\n0.2041559\r\n-29.77087\r\n0.9950998\r\n0.819504\r\n2\r\n7\r\n0\r\n31.04836\r\n1\r\n6\r\n6\r\n1.545824\r\n0.1237708\r\n1.3101264\r\n0.0343844\r\n0.5337555\r\n-29.80001\r\n0.8797944\r\n0.819504\r\n2\r\n5\r\n0\r\n31.09643\r\n\r\nHow do our priors and posteriors compare?\r\n\r\n\r\nlibrary(ggridges)\r\ntidybayes::tidy_draws(bird_posterior) |> \r\n  select(.draw, b_Intercept:prior_b_mother_mass_g_cen) |> \r\n  pivot_longer(-.draw) |> \r\n  ggplot(aes(x = value, y = name)) + geom_density_ridges()\r\n\r\n\r\n\r\n\r\nCan we draw the regression line?\r\n\r\n\r\naverage_mom <- mean(mother_masses_g)\r\n\r\nrange(imaginary_birds_centered$mother_mass_g_cen)\r\n\r\n\r\n[1] -6.025202  4.265214\r\n\r\ntibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, \r\n                                             n = 10)) |> \r\n  tidybayes::add_epred_draws(bird_posterior) |> \r\n  ungroup() |> \r\n  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .epred)) + \r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \r\n  geom_point(aes(x = mother_masses_g, y = nestlings),\r\n             data = imaginary_birds_centered, pch = 21,\r\n             fill = \"orange\", size = 3)\r\n\r\n\r\n\r\n\r\nlet’s also try drawing the prediction intervals\r\n\r\n\r\naverage_mom <- mean(mother_masses_g)\r\n\r\nrange(imaginary_birds_centered$mother_mass_g_cen)\r\n\r\n\r\n[1] -6.025202  4.265214\r\n\r\ntibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, \r\n                                             n = 10)) |> \r\n  tidybayes::add_predicted_draws(bird_posterior) |> \r\n  ungroup() |> \r\n  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .prediction)) + \r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \r\n  geom_point(aes(x = mother_masses_g, y = nestlings),\r\n             data = imaginary_birds_centered, pch = 21,\r\n             fill = \"orange\", size = 3)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-24-day-2-hierarchical-and-nonlinear-models/Day2_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-03-24T13:26:49-04:00",
    "input_file": "Day2.knit.md"
  }
]
